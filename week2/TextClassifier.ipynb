{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8a_TDA0eOtn"
   },
   "source": [
    "# Intro to simple text classification in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX-_2g9yeOtp"
   },
   "outputs": [],
   "source": [
    "# Do our imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential #base keras model\n",
    "from keras.layers import Dense, Activation #dense = fully connected layer\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRW0av17eOtt"
   },
   "outputs": [],
   "source": [
    "#if any of these give you problems, make sure you've installed all libraries used (pandas, sklearn, and matplot lib)\n",
    "# using conda install or pip install\n",
    "# see the moodle page \"Instructions for setting up and using Python and Jupyter\" for more info on how to do this\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwhcWbk_eOtt"
   },
   "source": [
    "## Loading a dataset\n",
    "\n",
    "We're going to use a small set of 1000 movie reviews from IMDB. [The original dataset can be found here.](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download) \n",
    "\n",
    "Here's how to load in the dataset into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM6fXWZafsod"
   },
   "outputs": [],
   "source": [
    "# This assumes the IMDBsubset.csv file lives in a directory called \"data\" which lives in the same directory as this notebook.\n",
    "# ***if you want to edit this notebook to use a different dataset, edit this to specify a different file:\n",
    "df = pd.read_csv(\"data/IMDBsubset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got the data read in! We've used a special data type called a \"data frame\", using the Pandas library, to store this data. Pandas makes working with data pretty convenient.\n",
    "\n",
    "Printing df will show you the data in a table-like format (specifically, it'll show you the first and last few rows of the table).\n",
    "\n",
    "**Note that a sentiment of \"1\" means \"positive\" and \"0\" means \"negative\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but if we want to read the full reviews (handy for later) then we can change our display options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) #show me everything in the column, even if it's long!\n",
    "df #Show me the first and last few examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something super simple to transform this into a dataset that we can send to a neural network. \n",
    "\n",
    "Similarly to the sentiment classification we discussed in lecture last week, we're going to represent each example (review) as a vector of word counts.\n",
    "\n",
    "The CountVectorizer object from sklearn allows us to make these word count vectors pretty easily. Once we do the counts, we'll store these in a new dataframe.\n",
    "\n",
    "\n",
    "The following code transforms a review dataframe to a word count dataframe called wordcounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a dataframe containing a column named \"review\" \n",
    "# such that each row becomes represented by a set of word counts, corresponding to the number of each term in the review\n",
    "\n",
    "#These next two lines perform word counting:\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=0.01)\n",
    "#stop_words='english' removes very common english words that are unlikely to be useful (e.g. \"and\", \"the\")\n",
    "#min_df=0.1 removes very rare words that are likely to be typos, uninformative, etc.\n",
    "# You can type ?CountVectorizer in its own cell to read its documentation\n",
    "#***Note that \"df['review']\" is used below because \"review\" is the name of the column containing our text in the dataframe\n",
    "#If you apply this to your own data, you may probably need to change this column name!\n",
    "matrix = vectorizer.fit_transform(df['review'])\n",
    "    \n",
    "#This line converts matrix into another dataframe, with column names corresponding to the word being counted\n",
    "data = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data #prints data to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can examine this dataset, e.g. to look at the column of counts for the word \"wonderful\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"wonderful\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or the word hate:\n",
    "data[\"hate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's split our dataset into training and test sets\n",
    "# Remember: X is for input, y is for output\n",
    "# The first argument of train_test_split is your training data (here, lives in \"data\" object you created using word counts)\n",
    "# The second argument of train_test_split is your labels/targets for the training data. This lives in the \"sentiment\" column of the original dataframe df we loaded from the file.\n",
    "# (***If you are using a different dataset, you'll need to change the name of this column to whatever it is in your dataset)\n",
    "# The test_size argument specifies % of data going into test set: here, 20% of the data goes into test set and 80% goes into training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, df['sentiment'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you ever want to learn more about a function, you can always use ? \n",
    "?train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykvGqf40eOtu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can examine it a bit using np.shape:\n",
    "np.shape(X_train) #What does our training data look like? It's 800 rows, with 1674 dimensions of input (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiLtJgDYeOtu"
   },
   "outputs": [],
   "source": [
    "#Now let's make a simple neural network with 1 hidden layer containing 10 neurons\n",
    "num_neurons = 10 # neurons in each layer\n",
    "model = Sequential()\n",
    "\n",
    "#Make the first (hidden) layer, which will have num_neurons neurons. Each neuron will get inputs from all columns of the dataframe, except sentiment\n",
    "#model.add(Dense(num_neurons, input_dim=len(data.columns)-1))\n",
    "model.add(Dense(num_neurons, input_dim=np.shape(X_train)[1]))\n",
    "model.add(Activation('sigmoid')) #Now we'll use a sigmoid activation function\n",
    "\n",
    "#Now let's add another layer for the output: A single sigmoid neuron.\n",
    "model.add(Dense(1)) \n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcylQdKjeOtv"
   },
   "outputs": [],
   "source": [
    "#Use compile() to set up our training\n",
    "\n",
    "# For loss, we'll use binary cross-entropy loss, \n",
    "# which is appropriate for a binary classification problem (0/1 for negative/positive)\n",
    "# ***If you edit this notebook to apply it to a multi-class classification problem, you'll need \n",
    "#    to change the loss to something like categorical_crossentropy, and you'll also need to change the\n",
    "#    encoding of the class to a one hot representation (see https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O2u0WC2eOtv"
   },
   "outputs": [],
   "source": [
    "#Train it!\n",
    "# Plus store history of training in a variable called \"history\"\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how training set and test set accuracy change with each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='training set accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'test set accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining model behaviour\n",
    "\n",
    "First, let's explore how we can apply the trained model to a specific example in our test data (or training data), to examine what it's done.\n",
    "\n",
    "We'll use the following code techniques:\n",
    "* We can apply the trained model to any example using the `.predict()` function\n",
    "* We can get the nth row from any dataframe using the `.iloc[[n]]` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For instance, let's make z the first test example:\n",
    "z = X_test.iloc[[0]]\n",
    "\n",
    "#and let's output the prediction for this example:\n",
    "model.predict(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this prediction will be somewhere between 0 and 1. This can be interpreted loosely as confidence: closer to 1 is more confident it is positive sentiment, closer to 0 is more confident it is negative sentiment.\n",
    "\n",
    "Let's compare this to the actual sentiment of the review, as stored in y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of this, we probably also want to look at the actual text review, which doesn't live in X_test but does live in the original dataframe we loaded from the CSV file, i.e. `df`. Since our `train_test_split` function has randomised the order of the data before splitting into training and testing sets, we need to get the id (row number) for `df` corresponding to this first test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = list(X_test.index) #gets the original indices in the df dataframe\n",
    "#test_ids[n] now refers to the id number of the nth test example\n",
    "originalReview = df.iloc[[test_ids[0]]].review\n",
    "originalReview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this with a few more examples and see what you find. If you're comfortable with python, can you think of a way to identify misclassified test examples and just print out those? Or, even better, find test examples that are confidently classified correctly, or test examples that are \"confidently\" misclassified, and examine those?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more fun, how about testing this classifier on our own new, fake \"reviews\"? Here's code to create an example of your own and apply the classifier to it. We'll have to first convert a string of text to a vector of word counts and put it in a dataframe, so here's a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns a text string into a dataframe example (***Note you'll need to change this from 'review' for your own dataset)\n",
    "def createExample(myText):\n",
    "\n",
    "    newExample = np.array([[myText]])\n",
    "    tdf = pd.DataFrame(newExample, columns=[\"review\"])\n",
    "    matrix = vectorizer.transform(tdf['review'])\n",
    "    newDf = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return newDf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's a text about zombies\n",
    "myText = \"This movie is about zombies zombies zombies\"\n",
    "t = createExample(myText) #When we print the dataframe, you see zombies' word count is 3:\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply the model to classify your new text:\n",
    "model.predict(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try writing some \"great\" and \"terrible\" reviews and see what happens to the classification outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(createExample(\"This movie is the worst it's terrible horrible\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore on your own\n",
    "\n",
    "Change the code above to explore:\n",
    "* Does changing the number of neurons in the hidden layer change the results? What happens to accuracy when you use 1 neuron? 100 neurons? \n",
    "* Try editing the neural network so that you have 2 hidden layers of 10 neurons each. What happens to accuracy? \n",
    "\n",
    "Investigating the model\n",
    "* Can you examine the model's performance on the test data to discover anything about what mistakes this model makes? Or anything about what types of reviews are easy to classify accurately?\n",
    "* Can you come up with your own, new examples of positive or negative reviews that illustrate the mistakes the model makes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Week 4.2-Neural network intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
